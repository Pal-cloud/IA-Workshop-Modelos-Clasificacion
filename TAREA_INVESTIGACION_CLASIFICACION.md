# Tarea: Investigaci√≥n y Desarrollo de Algoritmos de Clasificaci√≥n en Machine Learning Supervisado

**Autor:** Paloma G√≥mez Salazar  
**Fecha:** 25 de Febrero de 2026

---

## Parte 1: Algoritmos de Clasificaci√≥n en ML Supervisado

### 1. ¬øQu√© es la clasificaci√≥n en Machine Learning y cu√°l es su prop√≥sito?

#### Definici√≥n
La **clasificaci√≥n** en Machine Learning es una t√©cnica de aprendizaje supervisado que consiste en predecir la categor√≠a o clase a la que pertenece una observaci√≥n bas√°ndose en sus caracter√≠sticas. El algoritmo aprende patrones de un conjunto de datos etiquetados (datos de entrenamiento) para luego poder asignar etiquetas a nuevos datos no vistos.

#### Prop√≥sito
El prop√≥sito principal de la clasificaci√≥n es:
- **Automatizar la toma de decisiones** bas√°ndose en patrones aprendidos
- **Categorizar datos** de manera eficiente y precisa
- **Predecir resultados discretos** (clases o categor√≠as)
- **Identificar patrones complejos** que ser√≠an dif√≠ciles de detectar manualmente

#### Ejemplo Pr√°ctico del Mundo Real

**Problema:** Detecci√≥n de Fraude en Transacciones Bancarias

**Contexto:**  
Los bancos procesan millones de transacciones diarias y necesitan identificar autom√°ticamente cu√°les son fraudulentas para proteger a sus clientes.

**Aplicaci√≥n:**
- **Datos de entrada:** Monto de la transacci√≥n, ubicaci√≥n, hora, tipo de comercio, historial del cliente, etc.
- **Clases:** "Transacci√≥n Leg√≠tima" vs "Transacci√≥n Fraudulenta"
- **Objetivo:** Clasificar cada transacci√≥n en tiempo real para bloquear operaciones sospechosas

**Beneficios:**
- Reducci√≥n de p√©rdidas econ√≥micas por fraude
- Protecci√≥n inmediata de los clientes
- An√°lisis de millones de transacciones en segundos
- Mejora continua del modelo con nuevos datos

---

### 2. ¬øCu√°l es la diferencia entre clasificaci√≥n binaria y multiclase?

#### Clasificaci√≥n Binaria

**Definici√≥n:**  
La clasificaci√≥n binaria es cuando el modelo predice entre **dos clases posibles** (generalmente representadas como 0 y 1, o positivo y negativo).

**Caracter√≠sticas:**
- Solo hay dos categor√≠as mutuamente excluyentes
- El resultado es binario: S√≠/No, Verdadero/Falso, Clase A/Clase B
- M√°s simple de implementar y evaluar

**Ejemplos:**

1. **Diagn√≥stico M√©dico**
   - Clases: "Enfermo" vs "Sano"
   - Aplicaci√≥n: Detectar si un paciente tiene una enfermedad espec√≠fica (ej: diabetes)

2. **Filtro de Spam**
   - Clases: "Spam" vs "No Spam"
   - Aplicaci√≥n: Clasificar correos electr√≥nicos entrantes

3. **Aprobaci√≥n de Cr√©dito**
   - Clases: "Aprobado" vs "Rechazado"
   - Aplicaci√≥n: Decidir si otorgar un pr√©stamo a un cliente

4. **Predicci√≥n de Abandono de Clientes (Churn)**
   - Clases: "Se quedar√°" vs "Se ir√°"
   - Aplicaci√≥n: Identificar clientes en riesgo de cancelar un servicio

**Cu√°ndo usar:**  
Cuando el problema tiene exactamente dos resultados posibles y necesitas una decisi√≥n clara entre dos opciones.

---

#### Clasificaci√≥n Multiclase

**Definici√≥n:**  
La clasificaci√≥n multiclase es cuando el modelo debe predecir entre **tres o m√°s clases posibles**.

**Caracter√≠sticas:**
- M√∫ltiples categor√≠as (‚â• 3)
- Una observaci√≥n pertenece a una sola clase
- M√°s compleja que la clasificaci√≥n binaria

**Ejemplos:**

1. **Reconocimiento de D√≠gitos Manuscritos**
   - Clases: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9 (10 clases)
   - Aplicaci√≥n: Leer c√≥digos postales o cheques bancarios

2. **Clasificaci√≥n de Tipos de Flores**
   - Clases: "Iris Setosa", "Iris Versicolor", "Iris Virginica"
   - Aplicaci√≥n: Identificar especies de plantas en bot√°nica

3. **Categorizaci√≥n de Noticias**
   - Clases: "Deportes", "Pol√≠tica", "Tecnolog√≠a", "Entretenimiento", "Econom√≠a"
   - Aplicaci√≥n: Organizar autom√°ticamente art√≠culos en un portal de noticias

4. **Clasificaci√≥n de Enfermedades**
   - Clases: M√∫ltiples tipos de c√°ncer o condiciones m√©dicas
   - Aplicaci√≥n: Diagn√≥stico diferencial en medicina

**Cu√°ndo usar:**  
Cuando el problema tiene tres o m√°s categor√≠as posibles y cada observaci√≥n pertenece a exactamente una de ellas.

---

#### Diferencias Clave

| Aspecto | Clasificaci√≥n Binaria | Clasificaci√≥n Multiclase |
|---------|----------------------|--------------------------|
| **N√∫mero de clases** | 2 | ‚â• 3 |
| **Complejidad** | Menor | Mayor |
| **Funci√≥n de activaci√≥n** | Sigmoid | Softmax |
| **M√©trica com√∫n** | AUC-ROC | Accuracy, F1 macro/micro |
| **Ejemplo** | Spam vs No Spam | Clasificar animales en gato, perro, p√°jaro |

---

### 3. ¬øCu√°les son los principales algoritmos de clasificaci√≥n supervisada?

#### üîπ Regresi√≥n Log√≠stica (Logistic Regression)

**Descripci√≥n:**  
A pesar de su nombre, la regresi√≥n log√≠stica es un algoritmo de **clasificaci√≥n** que predice la probabilidad de que una observaci√≥n pertenezca a una clase espec√≠fica. Utiliza la funci√≥n sigmoide para mapear las predicciones a valores entre 0 y 1.

**Funcionamiento:**
- Calcula una combinaci√≥n lineal de las caracter√≠sticas
- Aplica la funci√≥n log√≠stica (sigmoide): œÉ(z) = 1 / (1 + e^(-z))
- Si la probabilidad es > 0.5 ‚Üí Clase 1, si no ‚Üí Clase 0

**Caracter√≠sticas:**
- **Ventajas:**
  - Simple e interpretable
  - R√°pido de entrenar
  - Proporciona probabilidades
  - Funciona bien con relaciones lineales
  - Pocos hiperpar√°metros
  
- **Desventajas:**
  - Asume linealidad entre caracter√≠sticas y log-odds
  - No captura relaciones no lineales complejas
  - Sensible a outliers

**Casos de uso ideales:**
- Clasificaci√≥n binaria con relaciones lineales
- Cuando se necesita interpretar las probabilidades
- Baseline para comparar modelos m√°s complejos
- Problemas con muchas caracter√≠sticas

**Ejemplo:** Predecir si un estudiante aprobar√° un examen bas√°ndose en horas de estudio.

---

#### üîπ √Årboles de Decisi√≥n (Decision Trees)

**Descripci√≥n:**  
Los √°rboles de decisi√≥n dividen el espacio de caracter√≠sticas mediante reglas if-then-else, creando una estructura jer√°rquica similar a un diagrama de flujo. Cada nodo interno representa una pregunta sobre una caracter√≠stica, y cada hoja representa una predicci√≥n de clase.

**Funcionamiento:**
1. Selecciona la mejor caracter√≠stica para dividir los datos (bas√°ndose en Gini o Entrop√≠a)
2. Crea ramas para cada valor posible de esa caracter√≠stica
3. Repite el proceso recursivamente para cada subconjunto
4. Se detiene cuando se alcanza un criterio (pureza, profundidad m√°xima, etc.)

**Criterios de divisi√≥n:**
- **Gini Impurity:** Mide la probabilidad de clasificar incorrectamente
- **Entropy (Informaci√≥n Gain):** Mide el desorden en los datos

**Caracter√≠sticas:**
- **Ventajas:**
  - Extremadamente interpretables (visualizaci√≥n clara)
  - No requiere normalizaci√≥n de datos
  - Maneja datos num√©ricos y categ√≥ricos
  - Captura relaciones no lineales
  - Identifica caracter√≠sticas importantes autom√°ticamente
  
- **Desventajas:**
  - Propenso al **overfitting**
  - Inestable (peque√±os cambios en datos ‚Üí √°rbol diferente)
  - Puede crear √°rboles sesgados con clases desbalanceadas
  - Menor precisi√≥n comparado con ensemble methods

**Casos de uso ideales:**
- Cuando se necesita explicabilidad
- Problemas con caracter√≠sticas mixtas (num√©ricas y categ√≥ricas)
- Exploraci√≥n inicial de datos
- Generar reglas de negocio interpretables

**Ejemplo:** Decidir si una familia adoptar√° una mascota bas√°ndose en tama√±o de casa, tiempo libre y presencia de ni√±os.

---

#### üîπ Support Vector Machine (SVM)

**Descripci√≥n:**  
SVM busca el hiperplano √≥ptimo que maximiza el margen entre las clases. El margen es la distancia entre el hiperplano y los puntos m√°s cercanos de cada clase (vectores de soporte).

**Funcionamiento:**
1. Encuentra el hiperplano que separa las clases
2. Maximiza la distancia (margen) a los puntos m√°s cercanos
3. Los puntos m√°s cercanos se llaman "vectores de soporte"
4. Puede usar kernels para problemas no lineales

**Tipos de Kernels:**
- **Lineal:** Para datos linealmente separables
- **Polin√≥mico:** Captura interacciones polin√≥micas
- **RBF (Radial Basis Function):** Para patrones complejos no lineales
- **Sigmoide:** Similar a redes neuronales

**Caracter√≠sticas:**
- **Ventajas:**
  - Efectivo en espacios de alta dimensionalidad
  - Robusto ante outliers (solo los vectores de soporte importan)
  - Versatilidad mediante kernels
  - Funciona bien con datos complejos no lineales
  - Eficiente en memoria (solo almacena vectores de soporte)
  
- **Desventajas:**
  - Costoso computacionalmente con grandes datasets
  - Dif√≠cil de interpretar (especialmente con kernels no lineales)
  - Sensible a la elecci√≥n del kernel y par√°metros
  - No proporciona estimaciones de probabilidad directamente
  - Requiere normalizaci√≥n de caracter√≠sticas

**Casos de uso ideales:**
- Clasificaci√≥n de texto y reconocimiento de caracteres
- Clasificaci√≥n de im√°genes
- Datos con alta dimensionalidad
- Cuando hay un margen claro de separaci√≥n

**Ejemplo:** Clasificar tipos de flores (Iris) bas√°ndose en longitud y anchura de p√©talos.

---

#### üîπ K-Nearest Neighbors (KNN)

**Descripci√≥n:**  
KNN es un algoritmo basado en instancias que clasifica nuevos puntos seg√∫n las clases de sus k vecinos m√°s cercanos. Es un m√©todo "lazy learning" porque no construye un modelo expl√≠cito, sino que almacena todos los datos de entrenamiento.

**Funcionamiento:**
1. Cuando llega un nuevo punto, calcula la distancia a todos los puntos de entrenamiento
2. Selecciona los K vecinos m√°s cercanos
3. Realiza votaci√≥n mayoritaria (clasificaci√≥n) o promedio (regresi√≥n)
4. Asigna la clase m√°s com√∫n entre los K vecinos

**M√©tricas de distancia:**
- **Euclidiana:** ‚àö(Œ£(xi - yi)¬≤) - m√°s com√∫n
- **Manhattan:** Œ£|xi - yi|
- **Minkowski:** Generalizaci√≥n de Euclidiana y Manhattan
- **Coseno:** Para similitud de textos

**Caracter√≠sticas:**
- **Ventajas:**
  - Simple de entender e implementar
  - No requiere entrenamiento (lazy learning)
  - Naturalmente multiclase
  - Se adapta a nuevos datos f√°cilmente
  - Efectivo para datos con fronteras no lineales
  
- **Desventajas:**
  - Lento en predicci√≥n con grandes datasets
  - Sensible a la escala de caracter√≠sticas (requiere normalizaci√≥n)
  - Sensible al ruido y outliers
  - Curse of dimensionality (mal rendimiento en alta dimensionalidad)
  - Requiere almacenar todos los datos de entrenamiento

**Selecci√≥n de K:**
- K peque√±o: M√°s sensible al ruido, fronteras complejas
- K grande: M√°s suave, puede ignorar patrones locales
- K √≥ptimo: Se determina mediante validaci√≥n cruzada

**Casos de uso ideales:**
- Sistemas de recomendaci√≥n
- Clasificaci√≥n de patrones simples
- Cuando los datos de entrenamiento son peque√±os
- Reconocimiento de patrones locales

**Ejemplo:** Clasificar frutas (manzanas vs naranjas) bas√°ndose en peso y di√°metro.

---

#### üîπ Naive Bayes

**Descripci√≥n:**  
Naive Bayes es un clasificador probabil√≠stico basado en el Teorema de Bayes. Asume independencia condicional entre las caracter√≠sticas (de ah√≠ "naive" o ingenuo), lo que simplifica los c√°lculos.

**Teorema de Bayes:**
```
P(Clase|Caracter√≠sticas) = P(Caracter√≠sticas|Clase) √ó P(Clase) / P(Caracter√≠sticas)
```

**Funcionamiento:**
1. Calcula la probabilidad a priori de cada clase
2. Para cada clase, calcula la probabilidad de observar las caracter√≠sticas
3. Aplica el Teorema de Bayes
4. Asigna la clase con mayor probabilidad posterior

**Variantes:**
- **Gaussian Naive Bayes:** Para caracter√≠sticas continuas (asume distribuci√≥n normal)
- **Multinomial Naive Bayes:** Para caracter√≠sticas discretas (conteos, frecuencias)
- **Bernoulli Naive Bayes:** Para caracter√≠sticas binarias

**Caracter√≠sticas:**
- **Ventajas:**
  - Extremadamente r√°pido en entrenamiento y predicci√≥n
  - Funciona bien con pocas muestras de entrenamiento
  - Excelente para clasificaci√≥n de texto (spam filtering, sentiment analysis)
  - Maneja alta dimensionalidad eficientemente
  - Proporciona probabilidades de clase
  - Simple de implementar
  
- **Desventajas:**
  - Asunci√≥n de independencia raramente se cumple en realidad
  - No captura interacciones entre caracter√≠sticas
  - Puede ser superado por modelos m√°s sofisticados
  - Sensible a caracter√≠sticas irrelevantes

**Casos de uso ideales:**
- Clasificaci√≥n de texto (an√°lisis de sentimiento, categorizaci√≥n de documentos)
- Filtrado de spam
- Diagn√≥sticos m√©dicos basados en s√≠ntomas
- Sistemas de recomendaci√≥n simples
- Cuando se necesita velocidad y simplicidad

**Ejemplo:** Clasificar correos electr√≥nicos como spam o no spam bas√°ndose en la frecuencia de palabras.

---

### 4. ¬øQu√© m√©tricas se utilizan para evaluar modelos de clasificaci√≥n?

Las m√©tricas de evaluaci√≥n son fundamentales para medir el rendimiento de un modelo de clasificaci√≥n. La elecci√≥n de la m√©trica depende del problema espec√≠fico y del costo de los diferentes tipos de errores.

---

#### üìä Accuracy (Exactitud)

**Definici√≥n:**  
Es la proporci√≥n de predicciones correctas sobre el total de predicciones realizadas.

**F√≥rmula:**
```
Accuracy = (TP + TN) / (TP + TN + FP + FN)

Donde:
- TP (True Positives): Predicciones positivas correctas
- TN (True Negatives): Predicciones negativas correctas
- FP (False Positives): Predicciones positivas incorrectas (Error Tipo I)
- FN (False Negatives): Predicciones negativas incorrectas (Error Tipo II)
```

**Interpretaci√≥n:**
- Valor entre 0 y 1 (o 0% a 100%)
- Accuracy = 1 significa predicciones perfectas
- Accuracy = 0.85 significa que el modelo acierta el 85% de las veces

**Ventajas:**
- F√°cil de entender e interpretar
- M√©trica intuitiva para comparar modelos
- Buena m√©trica general de rendimiento

**Desventajas y Limitaciones:**
- **Paradoja de la Accuracy con clases desbalanceadas:**
  
  **Ejemplo:** Dataset de fraude bancario
  - 9,900 transacciones leg√≠timas (99%)
  - 100 transacciones fraudulentas (1%)
  
  Un modelo que prediga "todo es leg√≠timo" tendr√≠a:
  - Accuracy = 99% ¬°Pero no detecta ning√∫n fraude!
  - Este modelo es in√∫til pero tiene alta accuracy

**Cu√°ndo usar:**
- ‚úÖ Cuando las clases est√°n balanceadas
- ‚úÖ Cuando todos los errores tienen el mismo costo
- ‚ùå NO usar con datasets desbalanceados
- ‚ùå NO usar cuando ciertos errores son m√°s cr√≠ticos que otros

---

#### üìä Precision (Precisi√≥n)

**Definici√≥n:**  
De todas las predicciones positivas que hizo el modelo, ¬øcu√°ntas fueron realmente positivas?

**F√≥rmula:**
```
Precision = TP / (TP + FP)
```

**Pregunta que responde:**  
"¬øQu√© tan confiable es el modelo cuando predice la clase positiva?"

**Interpretaci√≥n:**
- Precision = 1 significa que todas las predicciones positivas fueron correctas
- Precision = 0.8 significa que el 80% de las predicciones positivas fueron correctas
- Alta precisi√≥n ‚Üí Pocos falsos positivos

**Ejemplo Pr√°ctico:**

**Problema:** Sistema de recomendaci√≥n de pel√≠culas
- TP: Pel√≠culas recomendadas que al usuario le gustaron
- FP: Pel√≠culas recomendadas que al usuario NO le gustaron

Alta precisi√≥n significa: "Cuando el sistema recomienda algo, probablemente te gustar√°"

**Cu√°ndo priorizar Precision:**
- üéØ Cuando los **falsos positivos son costosos**
- **Ejemplo 1 - Marketing:** Enviar emails promocionales solo a clientes realmente interesados (evitar molestar a usuarios)
- **Ejemplo 2 - Medicina:** Confirmar un diagn√≥stico grave antes de tratamiento invasivo
- **Ejemplo 3 - Sistema judicial:** Acusar a alguien de un crimen (mejor no acusar inocentes)
- **Ejemplo 4 - Spam filter:** Marcar emails como spam (no queremos que emails importantes vayan a spam)

---

#### üìä Recall (Sensibilidad / Sensitivity / True Positive Rate)

**Definici√≥n:**  
De todos los casos realmente positivos, ¬øcu√°ntos detect√≥ correctamente el modelo?

**F√≥rmula:**
```
Recall = TP / (TP + FN)
```

**Pregunta que responde:**  
"¬øQu√© tan bueno es el modelo para encontrar todos los casos positivos?"

**Interpretaci√≥n:**
- Recall = 1 significa que el modelo encontr√≥ todos los casos positivos
- Recall = 0.8 significa que el modelo detect√≥ el 80% de los casos positivos
- Alto recall ‚Üí Pocos falsos negativos

**Ejemplo Pr√°ctico:**

**Problema:** Detecci√≥n de tumores cancer√≠genos
- TP: Tumores cancer√≠genos detectados correctamente
- FN: Tumores cancer√≠genos NO detectados (ERROR GRAVE)

Alto recall significa: "El modelo no se pierde casos de c√°ncer"

**Cu√°ndo priorizar Recall:**
- üéØ Cuando los **falsos negativos son costosos**
- **Ejemplo 1 - Medicina:** Detecci√≥n de enfermedades graves (no queremos perder ning√∫n caso)
- **Ejemplo 2 - Seguridad:** Detecci√≥n de fraude bancario (mejor alertar de m√°s que perder un fraude)
- **Ejemplo 3 - Rescates:** Detectar personas en emergencias (no queremos dejar a nadie sin ayuda)
- **Ejemplo 4 - Control de calidad:** Detectar productos defectuosos (no queremos que salgan al mercado)

---

#### ‚öñÔ∏è Trade-off entre Precision y Recall

Existe una tensi√≥n natural entre estas dos m√©tricas:

**Aumentar Precision ‚Üí Disminuye Recall**
- Ser m√°s estricto en las predicciones positivas
- Menos falsos positivos, pero m√°s falsos negativos

**Aumentar Recall ‚Üí Disminuye Precision**
- Ser m√°s permisivo en las predicciones positivas
- Menos falsos negativos, pero m√°s falsos positivos

**Ejemplo visual:**
```
Sistema de detecci√≥n de spam MUY ESTRICTO:
- Precision alta (poco spam leg√≠timo marcado como spam)
- Recall bajo (algunos spams se cuelan como leg√≠timos)

Sistema de detecci√≥n de spam MUY PERMISIVO:
- Precision baja (muchos emails leg√≠timos marcados como spam)
- Recall alto (casi todo el spam es detectado)
```

---

#### üìä F1-Score

**Definici√≥n:**  
Es la media arm√≥nica entre Precision y Recall. Proporciona un balance entre ambas m√©tricas.

**F√≥rmula:**
```
F1-Score = 2 √ó (Precision √ó Recall) / (Precision + Recall)
```

**¬øPor qu√© media arm√≥nica y no media aritm√©tica?**
- La media arm√≥nica penaliza valores extremos
- Si Precision o Recall es muy bajo, el F1-Score tambi√©n ser√° bajo
- Ejemplo: Precision=1.0, Recall=0.1
  - Media aritm√©tica = 0.55 (enga√±osamente alto)
  - F1-Score = 0.18 (refleja el desequilibrio)

**Interpretaci√≥n:**
- F1 = 1 es perfecto (precision y recall perfectos)
- F1 alto indica buen balance entre precision y recall
- F1 bajo indica que al menos una de las m√©tricas es mala

**Variantes:**

**F-Beta Score:** Permite dar m√°s peso a Precision o Recall
```
FŒ≤ = (1 + Œ≤¬≤) √ó (Precision √ó Recall) / (Œ≤¬≤ √ó Precision + Recall)

- Œ≤ < 1: Mayor peso a Precision
- Œ≤ > 1: Mayor peso a Recall
- Œ≤ = 1: F1-Score (balance igual)
- Œ≤ = 2: F2-Score (doble peso a Recall)
```

**Cu√°ndo usar F1-Score:**
- ‚úÖ Clases desbalanceadas
- ‚úÖ Cuando necesitas balance entre Precision y Recall
- ‚úÖ Cuando ambos tipos de errores son importantes
- ‚úÖ Para comparar modelos de manera equilibrada

---

#### üìä Matriz de Confusi√≥n (Confusion Matrix)

**Definici√≥n:**  
Es una tabla que visualiza el rendimiento de un modelo de clasificaci√≥n mostrando las predicciones vs los valores reales.

**Estructura (Clasificaci√≥n Binaria):**

```
                      Predicci√≥n
                   Negativo  Positivo
Real  Negativo  |    TN    |    FP    |
      Positivo  |    FN    |    TP    |
```

**Componentes:**
- **True Negatives (TN):** Correctamente predijo clase negativa
- **True Positives (TP):** Correctamente predijo clase positiva
- **False Positives (FP):** Predijo positivo pero era negativo (Error Tipo I)
- **False Negatives (FN):** Predijo negativo pero era positivo (Error Tipo II)

**Ejemplo Num√©rico:**

**Problema:** Detecci√≥n de c√°ncer en 100 pacientes

```
                      Predicci√≥n
                   Sano    Enfermo
Real  Sano      |   85   |    5    | = 90 pacientes sanos
      Enfermo   |   2    |    8    | = 10 pacientes enfermos
                 ----------------------
                   87       13
```

**C√°lculos:**
- **Accuracy** = (85 + 8) / 100 = 0.93 (93%)
- **Precision** = 8 / (8 + 5) = 0.615 (61.5%)
- **Recall** = 8 / (8 + 2) = 0.80 (80%)
- **F1-Score** = 2 √ó (0.615 √ó 0.80) / (0.615 + 0.80) = 0.696

**Interpretaci√≥n:**
- 85 personas sanas correctamente identificadas ‚úÖ
- 8 personas enfermas correctamente identificadas ‚úÖ
- 5 personas sanas incorrectamente diagnosticadas como enfermas ‚ö†Ô∏è (tratamientos innecesarios)
- 2 personas enfermas NO detectadas ‚õî (¬°MUY GRAVE!)

**Para Clasificaci√≥n Multiclase:**

Ejemplo con 3 clases (Gato, Perro, P√°jaro):

```
           Predicci√≥n
           Gato  Perro  P√°jaro
Real Gato    45    3      2
     Perro    2   38      5
     P√°jaro   1    4     40
```

**Ventajas de la Matriz de Confusi√≥n:**
- Visualizaci√≥n clara de d√≥nde se equivoca el modelo
- Identifica patrones de confusi√≥n entre clases espec√≠ficas
- Base para calcular todas las dem√°s m√©tricas
- √ötil para clases m√∫ltiples

---

#### üìä ROC-AUC (Receiver Operating Characteristic - Area Under Curve)

**ROC Curve (Curva ROC):**

**Definici√≥n:**  
Es una representaci√≥n gr√°fica que muestra el rendimiento de un modelo de clasificaci√≥n en todos los umbrales de decisi√≥n posibles.

**Ejes:**
- **Eje Y:** TPR (True Positive Rate) = Recall = TP / (TP + FN)
- **Eje X:** FPR (False Positive Rate) = FP / (FP + TN)

**Interpretaci√≥n de la Curva:**
- Muestra el trade-off entre TPR (beneficio) y FPR (costo)
- Cada punto de la curva representa un umbral diferente
- Cuanto m√°s cerca est√© la curva de la esquina superior izquierda, mejor

**AUC (Area Under the Curve):**

**Definici√≥n:**  
Es el √°rea bajo la curva ROC. Representa la probabilidad de que el modelo clasifique correctamente un ejemplo positivo aleatorio por encima de un ejemplo negativo aleatorio.

**Interpretaci√≥n de valores:**
```
AUC = 1.0   ‚Üí Modelo perfecto (clasificaci√≥n perfecta)
AUC = 0.9-1.0 ‚Üí Excelente
AUC = 0.8-0.9 ‚Üí Muy bueno
AUC = 0.7-0.8 ‚Üí Bueno
AUC = 0.6-0.7 ‚Üí Regular
AUC = 0.5   ‚Üí Modelo in√∫til (como lanzar una moneda)
AUC < 0.5   ‚Üí Peor que aleatorio (predicciones invertidas)
```

**Ventajas de ROC-AUC:**
- **Independiente del umbral:** Eval√∫a el modelo en todos los umbrales posibles
- **Robusta ante clases desbalanceadas:** A diferencia de accuracy
- **Comparaci√≥n de modelos:** F√°cil de comparar varios modelos
- **Interpretaci√≥n probabil√≠stica:** Mide capacidad discriminativa

**Desventajas:**
- Puede ser optimista con clases muy desbalanceadas
- No proporciona informaci√≥n sobre calibraci√≥n de probabilidades
- Puede enmascarar problemas en clases espec√≠ficas

**Cu√°ndo usar ROC-AUC:**
- ‚úÖ Comparar m√∫ltiples modelos
- ‚úÖ Cuando no hay un umbral de decisi√≥n fijo
- ‚úÖ Para evaluar modelos que producen probabilidades
- ‚úÖ En clasificaci√≥n binaria
- ‚ö†Ô∏è Con precauci√≥n en datasets muy desbalanceados (considerar Precision-Recall curve)

**Alternativa para clases desbalanceadas:**
**PR-AUC (Precision-Recall AUC):** M√°s informativa cuando la clase positiva es rara

---

#### üìä Resumen Comparativo de M√©tricas

| M√©trica | Cu√°ndo Usar | Ventaja Principal | Limitaci√≥n Principal |
|---------|-------------|-------------------|----------------------|
| **Accuracy** | Clases balanceadas | F√°cil interpretaci√≥n | Enga√±osa con desbalance |
| **Precision** | Evitar FP costosos | Minimiza falsos positivos | Ignora FN |
| **Recall** | Evitar FN costosos | Minimiza falsos negativos | Ignora FP |
| **F1-Score** | Balance FP y FN | Combina Precision y Recall | Puede enmascarar debilidades |
| **Confusion Matrix** | An√°lisis detallado | Visualiza todos los errores | No es un n√∫mero √∫nico |
| **ROC-AUC** | Comparar modelos | Independiente umbral | Optimista con desbalance |

---

### 5. ¬øQu√© es el feature engineering y feature selection en clasificaci√≥n?

#### üîß Feature Engineering (Ingenier√≠a de Caracter√≠sticas)

**Definici√≥n:**  
Feature Engineering es el proceso de crear nuevas caracter√≠sticas (features) o transformar las existentes para mejorar el rendimiento del modelo de Machine Learning. Es considerado uno de los aspectos m√°s importantes y creativos del ML.

**Objetivo:**
Extraer informaci√≥n m√°s relevante de los datos crudos para que el modelo pueda aprender patrones m√°s f√°cilmente.

---

##### **T√©cnicas Comunes de Feature Engineering**

**1. Creaci√≥n de Caracter√≠sticas Num√©ricas:**

**a) Transformaciones Matem√°ticas:**
```python
# De datos crudos a caracter√≠sticas √∫tiles
df['area'] = df['largo'] * df['ancho']
df['IMC'] = df['peso'] / (df['altura'] ** 2)
df['ratio_precio_superficie'] = df['precio'] / df['metros_cuadrados']
df['log_ingreso'] = np.log(df['ingreso'] + 1)
df['sqrt_edad'] = np.sqrt(df['edad'])
```

**Ejemplo Real - Ventas:**
```python
# Datos originales: ventas_mes, clientes_mes
# Caracter√≠sticas creadas:
df['venta_promedio_por_cliente'] = df['ventas_mes'] / df['clientes_mes']
df['crecimiento_ventas'] = df['ventas_mes'] - df['ventas_mes_anterior']
df['tasa_crecimiento'] = (df['ventas_mes'] / df['ventas_mes_anterior']) - 1
```

**b) Caracter√≠sticas Temporales:**
```python
# De datetime a m√∫ltiples caracter√≠sticas
df['fecha'] = pd.to_datetime(df['fecha'])
df['a√±o'] = df['fecha'].dt.year
df['mes'] = df['fecha'].dt.month
df['dia_semana'] = df['fecha'].dt.dayofweek
df['dia_mes'] = df['fecha'].dt.day
df['trimestre'] = df['fecha'].dt.quarter
df['es_fin_de_semana'] = df['dia_semana'].isin([5, 6]).astype(int)
df['es_festivo'] = df['fecha'].isin(dias_festivos).astype(int)
df['hora_del_dia'] = df['fecha'].dt.hour
df['periodo_dia'] = pd.cut(df['hora_del_dia'], bins=[0,6,12,18,24], 
                           labels=['madrugada','ma√±ana','tarde','noche'])
```

**Ejemplo Real - E-commerce:**
```python
# Predecir compras en una tienda online
df['dias_desde_ultima_compra'] = (fecha_actual - df['fecha_ultima_compra']).dt.days
df['dias_desde_registro'] = (fecha_actual - df['fecha_registro']).dt.days
df['compras_por_mes_actividad'] = df['total_compras'] / (df['dias_desde_registro'] / 30)
```

**c) Agregaciones:**
```python
# Caracter√≠sticas agregadas por grupo
clientes_agg = transacciones.groupby('cliente_id').agg({
    'monto': ['sum', 'mean', 'std', 'min', 'max'],
    'transaccion_id': 'count',
    'fecha': ['min', 'max']
}).reset_index()

clientes_agg.columns = ['cliente_id', 'monto_total', 'monto_promedio',
                        'monto_std', 'monto_min', 'monto_max',
                        'num_transacciones', 'primera_transaccion', 
                        'ultima_transaccion']
```

**2. Encoding de Variables Categ√≥ricas:**

**a) One-Hot Encoding:**
```python
# Para categor√≠as nominales sin orden
# Original: ['Rojo', 'Verde', 'Azul']
# Resultado:
#   color_Rojo  color_Verde  color_Azul
#      1           0            0
#      0           1            0
#      0           0            1

pd.get_dummies(df, columns=['color'], drop_first=True)
```

**b) Label Encoding:**
```python
# Para categor√≠as ordinales con orden
# Educaci√≥n: ['Primaria', 'Secundaria', 'Universidad', 'Posgrado']
educacion_map = {
    'Primaria': 1,
    'Secundaria': 2,
    'Universidad': 3,
    'Posgrado': 4
}
df['educacion_encoded'] = df['educacion'].map(educacion_map)
```

**c) Target Encoding:**
```python
# Codificar bas√°ndose en la variable objetivo
# √ötil para categor√≠as con alta cardinalidad
ciudad_target_mean = df.groupby('ciudad')['compra'].mean()
df['ciudad_encoded'] = df['ciudad'].map(ciudad_target_mean)
```

**d) Frequency Encoding:**
```python
# Codificar por frecuencia de aparici√≥n
freq = df['marca'].value_counts(normalize=True)
df['marca_freq'] = df['marca'].map(freq)
```

**3. Binning (Discretizaci√≥n):**
```python
# Convertir variables continuas en categ√≥ricas
df['edad_grupo'] = pd.cut(df['edad'], 
                          bins=[0, 18, 35, 50, 65, 100],
                          labels=['Menor', 'Joven', 'Adulto', 'Senior', 'Anciano'])

df['ingreso_nivel'] = pd.qcut(df['ingreso'], 
                               q=4, 
                               labels=['Bajo', 'Medio-Bajo', 'Medio-Alto', 'Alto'])
```

**4. Interacciones entre Caracter√≠sticas:**
```python
# Crear caracter√≠sticas combinadas
df['edad_x_ingreso'] = df['edad'] * df['ingreso']
df['area_x_habitaciones'] = df['area'] * df['habitaciones']

# Interacciones polin√≥micas
from sklearn.preprocessing import PolynomialFeatures
poly = PolynomialFeatures(degree=2, include_bias=False)
features_poly = poly.fit_transform(df[['feature1', 'feature2']])
```

**5. Caracter√≠sticas de Texto:**

```python
# De texto crudo a caracter√≠sticas num√©ricas
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer

# Bag of Words
vectorizer = CountVectorizer(max_features=1000)
bow_features = vectorizer.fit_transform(df['texto'])

# TF-IDF
tfidf = TfidfVectorizer(max_features=1000, ngram_range=(1,2))
tfidf_features = tfidf.fit_transform(df['texto'])

# Caracter√≠sticas adicionales
df['longitud_texto'] = df['texto'].str.len()
df['num_palabras'] = df['texto'].str.split().str.len()
df['num_mayusculas'] = df['texto'].str.count('[A-Z]')
df['num_signos_exclamacion'] = df['texto'].str.count('!')
df['tiene_url'] = df['texto'].str.contains('http').astype(int)
```

**6. Scaling y Normalizaci√≥n:**
```python
from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler

# StandardScaler: media=0, std=1
scaler = StandardScaler()
df_scaled = scaler.fit_transform(df[numeric_features])

# MinMaxScaler: rango [0,1]
scaler = MinMaxScaler()
df_normalized = scaler.fit_transform(df[numeric_features])

# RobustScaler: robusto a outliers
scaler = RobustScaler()
df_robust = scaler.fit_transform(df[numeric_features])
```

---

##### **Importancia del Feature Engineering**

**Impacto en el Rendimiento:**
> "Coming up with features is difficult, time-consuming, requires expert knowledge. 
> Applied machine learning is basically feature engineering." 
> ‚Äî Andrew Ng

**Estad√≠sticas:**
- Puede mejorar la accuracy de 70% a 95% en algunos problemas
- Modelos simples con buenas caracter√≠sticas > Modelos complejos con caracter√≠sticas malas
- 80% del tiempo en ML se gasta en feature engineering

**Ejemplo Real:**

**Problema:** Predecir cancelaci√≥n de suscripciones (Churn)

**Caracter√≠sticas Originales:**
- fecha_registro, fecha_ultima_visita, num_compras, monto_total

**Features Engineered:**
```python
# Mucho m√°s poderosas:
- dias_inactivo = (hoy - fecha_ultima_visita).days
- frecuencia_compras = num_compras / meses_activo
- ticket_promedio = monto_total / num_compras
- tendencia_compras = (compras_ultimos_3_meses - compras_3_meses_anteriores)
- ratio_productos_premium = compras_premium / num_compras
- engagement_score = visitas_mes * (compras_mes / visitas_mes)
```

**Resultado:** Accuracy aument√≥ de 72% a 89%

---

#### üéØ Feature Selection (Selecci√≥n de Caracter√≠sticas)

**Definici√≥n:**  
Feature Selection es el proceso de seleccionar el subconjunto m√°s relevante de caracter√≠sticas que contribuyen significativamente a la predicci√≥n, eliminando caracter√≠sticas redundantes o irrelevantes.

**¬øPor qu√© es importante?**

**Problemas con demasiadas caracter√≠sticas:**
- **Overfitting:** El modelo memoriza ruido
- **Curse of Dimensionality:** Rendimiento degradado en alta dimensionalidad
- **Mayor tiempo de entrenamiento:** M√°s caracter√≠sticas = m√°s lento
- **Dificultad de interpretaci√≥n:** Modelos complejos dif√≠ciles de explicar
- **Correlaciones espurias:** Caracter√≠sticas irrelevantes que parecen √∫tiles por azar

**Beneficios de Feature Selection:**
- ‚úÖ Mejora la generalizaci√≥n del modelo
- ‚úÖ Reduce overfitting
- ‚úÖ Acelera el entrenamiento
- ‚úÖ Mejora la interpretabilidad
- ‚úÖ Reduce ruido

---

##### **M√©todos de Feature Selection**

**1. Filter Methods (M√©todos de Filtro)**

**Caracter√≠stica:** Independientes del modelo, usan estad√≠sticas.

**a) Correlaci√≥n:**
```python
# Eliminar caracter√≠sticas con alta correlaci√≥n entre s√≠
correlation_matrix = df.corr()

# Encontrar pares altamente correlacionados
high_corr = []
for i in range(len(correlation_matrix.columns)):
    for j in range(i):
        if abs(correlation_matrix.iloc[i, j]) > 0.9:
            high_corr.append(correlation_matrix.columns[i])

# Eliminar caracter√≠sticas redundantes
df_reduced = df.drop(columns=high_corr)
```

**b) Chi-Squared Test:**
```python
from sklearn.feature_selection import SelectKBest, chi2

# Para caracter√≠sticas categ√≥ricas
selector = SelectKBest(chi2, k=10)  # Seleccionar top 10
X_selected = selector.fit_transform(X, y)
selected_features = X.columns[selector.get_support()]
```

**c) Variance Threshold:**
```python
from sklearn.feature_selection import VarianceThreshold

# Eliminar caracter√≠sticas con baja varianza
selector = VarianceThreshold(threshold=0.1)
X_high_variance = selector.fit_transform(X)
```

**d) Mutual Information:**
```python
from sklearn.feature_selection import mutual_info_classif

# Mide dependencia no lineal con variable objetivo
mi_scores = mutual_info_classif(X, y)
mi_scores = pd.Series(mi_scores, index=X.columns)
mi_scores.sort_values(ascending=False)
```

**2. Wrapper Methods (M√©todos de Envoltura)**

**Caracter√≠stica:** Usan el modelo para evaluar subconjuntos de caracter√≠sticas.

**a) Forward Selection:**
```python
# Agregar caracter√≠sticas una por una
def forward_selection(X, y, model):
    selected_features = []
    remaining_features = list(X.columns)
    
    while remaining_features:
        best_score = 0
        best_feature = None
        
        for feature in remaining_features:
            test_features = selected_features + [feature]
            X_test = X[test_features]
            score = cross_val_score(model, X_test, y, cv=5).mean()
            
            if score > best_score:
                best_score = score
                best_feature = feature
        
        if best_feature:
            selected_features.append(best_feature)
            remaining_features.remove(best_feature)
        else:
            break
    
    return selected_features
```

**b) Backward Elimination:**
```python
# Eliminar caracter√≠sticas una por una
def backward_elimination(X, y, model, threshold=0.01):
    features = list(X.columns)
    
    while len(features) > 1:
        X_subset = X[features]
        scores = cross_val_score(model, X_subset, y, cv=5)
        baseline_score = scores.mean()
        
        worst_feature = None
        smallest_drop = threshold
        
        for feature in features:
            test_features = [f for f in features if f != feature]
            X_test = X[test_features]
            score = cross_val_score(model, X_test, y, cv=5).mean()
            drop = baseline_score - score
            
            if drop < smallest_drop:
                smallest_drop = drop
                worst_feature = feature
        
        if worst_feature:
            features.remove(worst_feature)
        else:
            break
    
    return features
```

**c) Recursive Feature Elimination (RFE):**
```python
from sklearn.feature_selection import RFE
from sklearn.ensemble import RandomForestClassifier

model = RandomForestClassifier()
rfe = RFE(estimator=model, n_features_to_select=10)
rfe.fit(X, y)

selected_features = X.columns[rfe.support_]
feature_ranking = pd.Series(rfe.ranking_, index=X.columns)
```

**3. Embedded Methods (M√©todos Embebidos)**

**Caracter√≠stica:** La selecci√≥n de caracter√≠sticas es parte del entrenamiento del modelo.

**a) Lasso (L1 Regularization):**
```python
from sklearn.linear_model import LassoCV

# Lasso elimina caracter√≠sticas poniendo coeficientes en 0
lasso = LassoCV(cv=5)
lasso.fit(X, y)

# Caracter√≠sticas seleccionadas (coeficientes no cero)
selected_features = X.columns[lasso.coef_ != 0]
```

**b) Feature Importance (Random Forest, XGBoost):**
```python
from sklearn.ensemble import RandomForestClassifier

# Entrenar modelo
rf = RandomForestClassifier(n_estimators=100)
rf.fit(X, y)

# Obtener importancia
feature_importance = pd.Series(rf.feature_importances_, index=X.columns)
feature_importance = feature_importance.sort_values(ascending=False)

# Seleccionar top N caracter√≠sticas
top_features = feature_importance.head(15).index
X_selected = X[top_features]
```

**c) XGBoost Feature Importance:**
```python
import xgboost as xgb

model = xgb.XGBClassifier()
model.fit(X, y)

# M√∫ltiples tipos de importancia
xgb.plot_importance(model, importance_type='weight')  # Frecuencia de uso
xgb.plot_importance(model, importance_type='gain')    # Ganancia promedio
xgb.plot_importance(model, importance_type='cover')   # Cobertura promedio
```

---

##### **Comparaci√≥n de M√©todos**

| M√©todo | Velocidad | Accuracy | Interpretabilidad | Cu√°ndo Usar |
|--------|-----------|----------|-------------------|-------------|
| **Filter** | Muy r√°pido | Buena | Alta | Datasets grandes, an√°lisis exploratorio |
| **Wrapper** | Lento | Mejor | Media | Datasets peque√±os/medianos, m√°ximo rendimiento |
| **Embedded** | R√°pido | Muy buena | Alta | Balance velocidad-rendimiento, modelos con regularizaci√≥n |

---

##### **Estrategia Recomendada**

**Pipeline Completo:**

```python
# 1. An√°lisis Exploratorio
- Identificar caracter√≠sticas con missing values
- Analizar distribuciones
- Detectar outliers

# 2. Feature Engineering
- Crear caracter√≠sticas derivadas
- Encoding de categ√≥ricas
- Scaling/normalizaci√≥n

# 3. Feature Selection - Fase 1 (Filter)
- Eliminar caracter√≠sticas con varianza cero
- Eliminar caracter√≠sticas altamente correlacionadas
- Calcular mutual information

# 4. Feature Selection - Fase 2 (Embedded)
- Entrenar Random Forest o XGBoost
- Seleccionar por feature importance

# 5. Feature Selection - Fase 3 (Wrapper - opcional)
- RFE para refinar selecci√≥n
- Validar con cross-validation

# 6. Evaluaci√≥n Final
- Comparar rendimiento con todas las caracter√≠sticas vs seleccionadas
- Analizar trade-offs interpretabilidad-rendimiento
```

**Ejemplo Completo:**

```python
from sklearn.preprocessing import StandardScaler
from sklearn.feature_selection import VarianceThreshold, SelectKBest, mutual_info_classif
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import cross_val_score

# 1. Feature Engineering
df['edad_al_cuadrado'] = df['edad'] ** 2
df['ingreso_log'] = np.log1p(df['ingreso'])
df['ratio_deuda_ingreso'] = df['deuda'] / df['ingreso']

# 2. Encoding
df_encoded = pd.get_dummies(df, columns=['ciudad', 'profesion'])

# 3. Scaling
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# 4. Remove low variance
var_selector = VarianceThreshold(threshold=0.1)
X_high_var = var_selector.fit_transform(X_scaled)

# 5. Mutual Information
mi_selector = SelectKBest(mutual_info_classif, k=20)
X_mi = mi_selector.fit_transform(X_high_var, y)

# 6. Feature Importance
rf = RandomForestClassifier()
rf.fit(X_mi, y)
importances = pd.Series(rf.feature_importances_)
top_features = importances.nlargest(15).index

# 7. Evaluaci√≥n
X_final = X_mi[:, top_features]
score = cross_val_score(rf, X_final, y, cv=5).mean()
print(f"Score con {len(top_features)} caracter√≠sticas: {score:.4f}")
```

---

### 7. ¬øQu√© son los hiperpar√°metros y c√≥mo se optimizan/ajustan?

#### üéõÔ∏è Hiperpar√°metros

**Definici√≥n:**  
Los hiperpar√°metros son configuraciones o ajustes que se establecen **antes** de entrenar un modelo de Machine Learning. A diferencia de los par√°metros (que el modelo aprende durante el entrenamiento), los hiperpar√°metros se deben definir de antemano y controlan el proceso de aprendizaje.

---

##### **Diferencia: Par√°metros vs Hiperpar√°metros**

| Aspecto | Par√°metros | Hiperpar√°metros |
|---------|------------|-----------------|
| **Definici√≥n** | Valores que el modelo aprende de los datos | Valores que configuran el proceso de aprendizaje |
| **Cu√°ndo se determinan** | Durante el entrenamiento | Antes del entrenamiento |
| **Ejemplo (Reg. Lineal)** | Coeficientes (weights), intercepto | Learning rate, regularizaci√≥n |
| **Ejemplo (Decision Tree)** | Reglas de divisi√≥n aprendidas | max_depth, min_samples_split |
| **Ejemplo (Neural Network)** | Pesos de las conexiones | N√∫mero de capas, neuronas, learning rate |
| **C√≥mo se obtienen** | Optimizaci√≥n autom√°tica (gradiente descendente) | B√∫squeda manual o autom√°tica |

---

##### **Ejemplos de Hiperpar√°metros por Algoritmo**

**1. Regresi√≥n Log√≠stica:**
```python
LogisticRegression(
    C=1.0,                    # Inverso de regularizaci√≥n (menor C = m√°s regularizaci√≥n)
    penalty='l2',             # Tipo de regularizaci√≥n: 'l1', 'l2', 'elasticnet'
    solver='lbfgs',           # Algoritmo de optimizaci√≥n
    max_iter=100,             # N√∫mero m√°ximo de iteraciones
    class_weight='balanced'   # Manejo de clases desbalanceadas
)
```

**2. Random Forest:**
```python
RandomForestClassifier(
    n_estimators=100,         # N√∫mero de √°rboles
    max_depth=None,           # Profundidad m√°xima de √°rboles
    min_samples_split=2,      # M√≠nimo de muestras para dividir nodo
    min_samples_leaf=1,       # M√≠nimo de muestras en hoja
    max_features='sqrt',      # N√∫mero de caracter√≠sticas por split
    bootstrap=True,           # Usar bootstrap samples
    criterion='gini'          # Criterio de divisi√≥n: 'gini' o 'entropy'
)
```

**3. Support Vector Machine:**
```python
SVC(
    C=1.0,                    # Par√°metro de regularizaci√≥n
    kernel='rbf',             # Tipo de kernel: 'linear', 'poly', 'rbf', 'sigmoid'
    gamma='scale',            # Coeficiente del kernel
    degree=3,                 # Grado del kernel polin√≥mico
    class_weight='balanced'   # Balance de clases
)
```

**4. K-Nearest Neighbors:**
```python
KNeighborsClassifier(
    n_neighbors=5,            # N√∫mero de vecinos
    weights='uniform',        # Funci√≥n de peso: 'uniform' o 'distance'
    metric='minkowski',       # M√©trica de distancia
    p=2                       # Par√°metro de Minkowski (2 = Euclidiana)
)
```

**5. XGBoost:**
```python
XGBClassifier(
    n_estimators=100,         # N√∫mero de √°rboles
    max_depth=6,              # Profundidad m√°xima
    learning_rate=0.1,        # Tasa de aprendizaje
    subsample=0.8,            # Fracci√≥n de muestras por √°rbol
    colsample_bytree=0.8,     # Fracci√≥n de caracter√≠sticas por √°rbol
    gamma=0,                  # Reducci√≥n de p√©rdida m√≠nima para split
    reg_alpha=0,              # Regularizaci√≥n L1
    reg_lambda=1              # Regularizaci√≥n L2
)
```

---

#### üîç Optimizaci√≥n de Hiperpar√°metros (Hyperparameter Tuning)

**Objetivo:**  
Encontrar la combinaci√≥n de hiperpar√°metros que maximiza el rendimiento del modelo en datos no vistos.

---

##### **1. Grid Search (B√∫squeda en Cuadr√≠cula)**

**Definici√≥n:**  
Prueba exhaustivamente todas las combinaciones posibles de hiperpar√°metros en una cuadr√≠cula predefinida.

**C√≥mo funciona:**
1. Defines un grid (cuadr√≠cula) con valores para cada hiperpar√°metro
2. El algoritmo prueba TODAS las combinaciones
3. Eval√∫a cada combinaci√≥n con validaci√≥n cruzada
4. Selecciona la mejor combinaci√≥n

**Implementaci√≥n en Python:**

```python
from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import RandomForestClassifier

# 1. Definir el modelo
model = RandomForestClassifier(random_state=42)

# 2. Definir el grid de hiperpar√°metros
param_grid = {
    'n_estimators': [50, 100, 200],           # 3 opciones
    'max_depth': [10, 20, 30, None],          # 4 opciones
    'min_samples_split': [2, 5, 10],          # 3 opciones
    'min_samples_leaf': [1, 2, 4],            # 3 opciones
    'max_features': ['sqrt', 'log2']          # 2 opciones
}
# Total combinaciones: 3 √ó 4 √ó 3 √ó 3 √ó 2 = 216 combinaciones

# 3. Configurar GridSearchCV
grid_search = GridSearchCV(
    estimator=model,
    param_grid=param_grid,
    cv=5,                        # Validaci√≥n cruzada de 5 folds
    scoring='accuracy',          # M√©trica a optimizar
    n_jobs=-1,                   # Usar todos los cores disponibles
    verbose=2                    # Mostrar progreso
)

# 4. Ejecutar b√∫squeda
grid_search.fit(X_train, y_train)

# 5. Resultados
print("Mejores hiperpar√°metros:", grid_search.best_params_)
print("Mejor score:", grid_search.best_score_)
print("Mejor modelo:", grid_search.best_estimator_)

# 6. Usar el mejor modelo
best_model = grid_search.best_estimator_
y_pred = best_model.predict(X_test)

# 7. Ver todos los resultados
results = pd.DataFrame(grid_search.cv_results_)
results.sort_values('rank_test_score').head(10)
```

**Ejemplo Completo con XGBoost:**

```python
from xgboost import XGBClassifier
from sklearn.model_selection import GridSearchCV

# Grid m√°s complejo
param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [3, 5, 7],
    'learning_rate': [0.01, 0.1, 0.3],
    'subsample': [0.7, 0.8, 0.9],
    'colsample_bytree': [0.7, 0.8, 0.9],
    'gamma': [0, 0.1, 0.2]
}

xgb_model = XGBClassifier(random_state=42)

grid_search = GridSearchCV(
    estimator=xgb_model,
    param_grid=param_grid,
    cv=5,
    scoring='f1',
    n_jobs=-1,
    verbose=2
)

grid_search.fit(X_train, y_train)

print(f"Mejores par√°metros: {grid_search.best_params_}")
print(f"Mejor F1-Score: {grid_search.best_score_:.4f}")
```

**Ventajas de Grid Search:**
- ‚úÖ Garantiza encontrar la mejor combinaci√≥n dentro del grid
- ‚úÖ Exhaustivo y sistem√°tico
- ‚úÖ F√°cil de entender e implementar
- ‚úÖ Reproducible

**Desventajas de Grid Search:**
- ‚ùå **Computacionalmente costoso** (crece exponencialmente)
- ‚ùå Puede tardar horas o d√≠as con grids grandes
- ‚ùå No escala bien con muchos hiperpar√°metros
- ‚ùå Puede desperdiciar recursos en zonas no prometedoras

**Cu√°ndo usar Grid Search:**
- Pocos hiperpar√°metros (2-4)
- Grid peque√±o
- Recursos computacionales disponibles
- Necesitas exploraci√≥n exhaustiva

---

##### **2. Random Search (B√∫squeda Aleatoria)**

**Definici√≥n:**  
En lugar de probar todas las combinaciones, Random Search muestrea aleatoriamente un n√∫mero fijo de combinaciones de hiperpar√°metros.

**Ventaja clave:**  
Estudios muestran que Random Search puede encontrar configuraciones igual de buenas que Grid Search en **mucho menos tiempo**.

**¬øPor qu√© funciona?**

**Teor√≠a (Bergstra & Bengio, 2012):**
- No todos los hiperpar√°metros son igualmente importantes
- Random Search explora m√°s valores para los hiperpar√°metros importantes
- Grid Search desperdicia evaluaciones en combinaciones similares

**Visualizaci√≥n conceptual:**
```
Grid Search (9 evaluaciones):
Hiperpar√°metro 1: [A, A, A, B, B, B, C, C, C]
Hiperpar√°metro 2: [X, Y, Z, X, Y, Z, X, Y, Z]

Random Search (9 evaluaciones):
Hiperpar√°metro 1: [A, D, B, F, C, E, A, G, H]  ‚Üê M√°s diversidad
Hiperpar√°metro 2: [X, W, Z, Y, T, V, S, U, R]  ‚Üê M√°s exploraci√≥n
```

**Implementaci√≥n en Python:**

```python
from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import randint, uniform
import numpy as np

# 1. Definir distribuciones de hiperpar√°metros
param_distributions = {
    'n_estimators': randint(50, 500),              # Enteros entre 50-500
    'max_depth': randint(3, 20),                   # Enteros entre 3-20
    'learning_rate': uniform(0.01, 0.3),           # Flotantes entre 0.01-0.31
    'subsample': uniform(0.5, 0.5),                # Flotantes entre 0.5-1.0
    'colsample_bytree': uniform(0.5, 0.5),         # Flotantes entre 0.5-1.0
    'gamma': uniform(0, 0.5),                      # Flotantes entre 0-0.5
    'min_child_weight': randint(1, 10),            # Enteros entre 1-10
    'reg_alpha': uniform(0, 1),                    # L1 regularization
    'reg_lambda': uniform(0, 1)                    # L2 regularization
}

# 2. Configurar RandomizedSearchCV
random_search = RandomizedSearchCV(
    estimator=XGBClassifier(random_state=42),
    param_distributions=param_distributions,
    n_iter=100,                  # N√∫mero de combinaciones a probar
    cv=5,
    scoring='roc_auc',
    n_jobs=-1,
    verbose=2,
    random_state=42
)

# 3. Ejecutar b√∫squeda
random_search.fit(X_train, y_train)

# 4. Resultados
print("Mejores hiperpar√°metros:", random_search.best_params_)
print("Mejor AUC:", random_search.best_score_)

# 5. Comparar top 10 configuraciones
results = pd.DataFrame(random_search.cv_results_)
top_10 = results.sort_values('rank_test_score').head(10)
print(top_10[['params', 'mean_test_score', 'std_test_score']])
```

**Ejemplo con Random Forest:**

```python
from sklearn.ensemble import RandomForestClassifier

param_distributions = {
    'n_estimators': randint(100, 1000),
    'max_depth': [None] + list(randint(10, 100).rvs(10)),
    'min_samples_split': randint(2, 20),
    'min_samples_leaf': randint(1, 10),
    'max_features': ['sqrt', 'log2', None],
    'bootstrap': [True, False],
    'criterion': ['gini', 'entropy']
}

random_search = RandomizedSearchCV(
    RandomForestClassifier(random_state=42),
    param_distributions=param_distributions,
    n_iter=50,
    cv=5,
    scoring='f1_weighted',
    n_jobs=-1,
    random_state=42
)

random_search.fit(X_train, y_train)
```

**Ventajas de Random Search:**
- ‚úÖ **Mucho m√°s r√°pido** que Grid Search
- ‚úÖ Escala mejor con muchos hiperpar√°metros
- ‚úÖ Explora m√°s diversidad de valores
- ‚úÖ Efectivo con presupuesto computacional limitado
- ‚úÖ Puede encontrar combinaciones inesperadas

**Desventajas de Random Search:**
- ‚ùå No garantiza encontrar el √≥ptimo global
- ‚ùå Puede requerir muchas iteraciones
- ‚ùå Aleatoriedad puede dar resultados inconsistentes

**Cu√°ndo usar Random Search:**
- Muchos hiperpar√°metros (>4)
- Espacio de b√∫squeda grande
- Recursos computacionales limitados
- Exploraci√≥n inicial

---

##### **3. Grid Search vs Random Search: Comparaci√≥n**

| Aspecto | Grid Search | Random Search |
|---------|-------------|---------------|
| **M√©todo** | Exhaustivo | Muestreo aleatorio |
| **Complejidad** | Exponencial | Lineal |
| **Tiempo** | Muy alto con muchos params | Controlable (n_iter) |
| **Cobertura** | Todos los valores predefinidos | Muestreo amplio del espacio |
| **√ìptimo** | Garantizado en el grid | No garantizado |
| **Escalabilidad** | Mala | Buena |
| **Mejor para** | Pocos hiperpar√°metros | Muchos hiperpar√°metros |

**Recomendaci√≥n pr√°ctica:**

```python
# Estrategia combinada:

# 1. Random Search amplio (exploraci√≥n)
random_search = RandomizedSearchCV(
    model,
    param_distributions=wide_distributions,
    n_iter=100,
    cv=5
)
random_search.fit(X_train, y_train)

# 2. Grid Search refinado (explotaci√≥n)
# Usar los mejores valores encontrados como centro
best_params = random_search.best_params_

refined_grid = {
    'n_estimators': [best_params['n_estimators']-50, 
                     best_params['n_estimators'],
                     best_params['n_estimators']+50],
    'max_depth': [best_params['max_depth']-2,
                  best_params['max_depth'],
                  best_params['max_depth']+2],
    # ... refinar otros par√°metros
}

grid_search = GridSearchCV(model, refined_grid, cv=5)
grid_search.fit(X_train, y_train)

final_model = grid_search.best_estimator_
```

---

##### **4. Otras T√©cnicas Avanzadas de Optimizaci√≥n**

**Bayesian Optimization:**
```python
from skopt import BayesSearchCV
from skopt.space import Real, Integer

# Usa modelos probabil√≠sticos para guiar la b√∫squeda
search_spaces = {
    'n_estimators': Integer(50, 500),
    'max_depth': Integer(3, 20),
    'learning_rate': Real(0.01, 0.3, prior='log-uniform'),
    'subsample': Real(0.5, 1.0),
}

bayes_search = BayesSearchCV(
    XGBClassifier(),
    search_spaces,
    n_iter=50,
    cv=5,
    n_jobs=-1
)
```

**Hyperband / ASHA:**
- Asignaci√≥n din√°mica de recursos
- Detiene configuraciones poco prometedoras temprano

**Optuna:**
```python
import optuna

def objective(trial):
    params = {
        'n_estimators': trial.suggest_int('n_estimators', 50, 500),
        'max_depth': trial.suggest_int('max_depth', 3, 20),
        'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.3),
    }
    
    model = XGBClassifier(**params)
    score = cross_val_score(model, X_train, y_train, cv=5).mean()
    return score

study = optuna.create_study(direction='maximize')
study.optimize(objective, n_trials=100)

print('Best params:', study.best_params)
```

---

##### **5. Mejores Pr√°cticas en Hyperparameter Tuning**

**‚úÖ DO (Hacer):**

1. **Usa validaci√≥n cruzada:**
   ```python
   GridSearchCV(cv=5)  # Siempre usar CV
   ```

2. **Separa test set:**
   ```python
   X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
   # Hacer tuning solo en X_train
   # Evaluar solo una vez en X_test al final
   ```

3. **Escala apropiadamente:**
   ```python
   # Incluir scaling en el pipeline
   from sklearn.pipeline import Pipeline
   
   pipeline = Pipeline([
       ('scaler', StandardScaler()),
       ('model', RandomForestClassifier())
   ])
   
   param_grid = {
       'model__n_estimators': [100, 200],
       'model__max_depth': [10, 20]
   }
   ```

4. **Empieza con rangos amplios:**
   ```python
   # Primera iteraci√≥n: amplio
   param_grid_wide = {
       'learning_rate': [0.001, 0.01, 0.1, 1.0]
   }
   
   # Segunda iteraci√≥n: refinado
   param_grid_refined = {
       'learning_rate': [0.08, 0.1, 0.12]
   }
   ```

5. **Monitorea tiempo de ejecuci√≥n:**
   ```python
   import time
   
   start = time.time()
   grid_search.fit(X_train, y_train)
   print(f"Tiempo: {time.time() - start:.2f}s")
   ```

**‚ùå DON'T (No hacer):**

1. ‚ùå No hacer overfitting en test set
2. ‚ùå No optimizar en todo el dataset
3. ‚ùå No ignorar el tiempo de entrenamiento
4. ‚ùå No usar demasiados hiperpar√°metros simult√°neamente
5. ‚ùå No olvidar el random_state para reproducibilidad

---

##### **Ejemplo Completo de Flujo de Trabajo:**

```python
from sklearn.model_selection import train_test_split, RandomizedSearchCV, GridSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix
import numpy as np

# 1. Split datos
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# 2. Random Search inicial (exploraci√≥n amplia)
param_dist_wide = {
    'n_estimators': randint(50, 500),
    'max_depth': randint(5, 50),
    'min_samples_split': randint(2, 20),
    'min_samples_leaf': randint(1, 10),
    'max_features': ['sqrt', 'log2', None]
}

random_search = RandomizedSearchCV(
    RandomForestClassifier(random_state=42),
    param_distributions=param_dist_wide,
    n_iter=50,
    cv=5,
    scoring='f1_weighted',
    n_jobs=-1,
    verbose=1,
    random_state=42
)

random_search.fit(X_train, y_train)
print(f"Mejor score Random Search: {random_search.best_score_:.4f}")
print(f"Mejores params: {random_search.best_params_}")

# 3. Grid Search refinado (explotaci√≥n local)
best_params = random_search.best_params_

param_grid_refined = {
    'n_estimators': [best_params['n_estimators']-50,
                     best_params['n_estimators'],
                     best_params['n_estimators']+50],
    'max_depth': [best_params['max_depth']-5,
                  best_params['max_depth'],
                  best_params['max_depth']+5],
    'min_samples_split': [max(2, best_params['min_samples_split']-2),
                          best_params['min_samples_split'],
                          best_params['min_samples_split']+2],
    'max_features': [best_params['max_features']]
}

grid_search = GridSearchCV(
    RandomForestClassifier(random_state=42),
    param_grid=param_grid_refined,
    cv=5,
    scoring='f1_weighted',
    n_jobs=-1,
    verbose=1
)

grid_search.fit(X_train, y_train)
print(f"Mejor score Grid Search: {grid_search.best_score_:.4f}")

# 4. Evaluaci√≥n final en test set
final_model = grid_search.best_estimator_
y_pred = final_model.predict(X_test)

print("\nResultados en Test Set:")
print(classification_report(y_test, y_pred))
print("\nMatriz de Confusi√≥n:")
print(confusion_matrix(y_test, y_pred))

# 5. Guardar modelo
import joblib
joblib.dump(final_model, 'best_model.pkl')
```

---

## üìö Conclusi√≥n

La clasificaci√≥n en Machine Learning es un campo fundamental que requiere comprender:

1. **Los algoritmos** y cu√°ndo usar cada uno
2. **Las m√©tricas** apropiadas para evaluar el rendimiento
3. **Feature engineering** para extraer informaci√≥n valiosa
4. **Feature selection** para mantener solo lo relevante
5. **Optimizaci√≥n de hiperpar√°metros** para maximizar el rendimiento

El √©xito en Machine Learning viene de:
- Entender profundamente el problema de negocio
- Experimentar con m√∫ltiples enfoques
- Validar rigurosamente los resultados
- Iterar y mejorar continuamente

---

## üìñ Referencias y Recursos Adicionales

### Documentaci√≥n Oficial
- [Scikit-Learn User Guide](https://scikit-learn.org/stable/user_guide.html)
- [XGBoost Documentation](https://xgboost.readthedocs.io/)
- [Keras Documentation](https://keras.io/)

### Libros Recomendados
- "Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow" - Aur√©lien G√©ron
- "Pattern Recognition and Machine Learning" - Christopher Bishop
- "The Elements of Statistical Learning" - Hastie, Tibshirani, Friedman

### Cursos Online
- [Coursera: Machine Learning - Andrew Ng](https://www.coursera.org/learn/machine-learning)
- [Fast.ai: Practical Deep Learning](https://www.fast.ai/)
- [Google: Machine Learning Crash Course](https://developers.google.com/machine-learning/crash-course)

### Papers Importantes
- Bergstra & Bengio (2012): "Random Search for Hyper-Parameter Optimization"
- Breiman (2001): "Random Forests"
- Cortes & Vapnik (1995): "Support-Vector Networks"

---

**Fin del Documento**

